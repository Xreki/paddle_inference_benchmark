#include "v2/inference_helper.h"
#include <iostream>
#include "utils/timer.h"

#define CHECK(stmt)                                            \
  do {                                                         \
    paddle_error __err__ = stmt;                               \
    if (__err__ != kPD_NO_ERROR) {                             \
      const char* str = paddle_error_string(__err__);          \
      fprintf(stderr, "%s (%d) in " #stmt "\n", str, __err__); \
      exit(__err__);                                           \
    }                                                          \
  } while (0)

void ReadBinaryFile(const char* file, void** buf, long* size) {
  FILE* fp = fopen(file, "r");
  if (fp) {
    if (fseek(fp, 0L, SEEK_END) == 0) {
      *size = ftell(fp);
      fseek(fp, 0L, SEEK_SET);
      *buf = malloc(*size);
      fread(*buf, 1, *size, fp);
    }
  }
  fclose(fp);
}

InferenceHelper::InferenceHelper(bool use_gpu)
    : machine_(nullptr), use_gpu_(use_gpu) {
  if (use_gpu) {
    char* argv[] = {const_cast<char*>("--use_gpu=True")};
    CHECK(paddle_init(1, (char**)argv));
  } else {
    char* argv[] = {const_cast<char*>("--use_gpu=False")};
    CHECK(paddle_init(1, (char**)argv));
  }
}

void InferenceHelper::Init(const std::string& merged_model_path) {
  if (merged_model_path.empty()) {
    return;
  }

  // Create a gradient machine for inference.
  Timer time("paddle_gradient_machine_create_for_inference_with_parameters");
  paddle_error error = kPD_NO_ERROR;

  long size = 0;
  void* buf = nullptr;
  ReadBinaryFile(merged_model_path.c_str(), &buf, &size);

  CHECK(paddle_gradient_machine_create_for_inference_with_parameters(
      &machine_, buf, size));

  free(buf);
}

void InferenceHelper::Init(const std::string& config_path,
                           const std::string& params_dirname) {
  if (config_path.empty()) {
    return;
  }

  // Reading config binary file. It is generated by `convert_protobin.sh`
  {
    Timer time("paddle_gradient_machine_create_for_inference");
    long size = 0;
    void* buf = nullptr;
    ReadBinaryFile(config_path.c_str(), &buf, &size);

    CHECK(paddle_gradient_machine_create_for_inference(&machine_, buf,
                                                       static_cast<int>(size)));

    free(buf);
  }

  if (params_dirname.empty()) {
    Timer time("paddle_gradient_machine_randomize_param");
    CHECK(paddle_gradient_machine_randomize_param(machine_));
  } else {
    Timer time("paddle_gradient_machine_load_parameter_from_disk");
    CHECK(paddle_gradient_machine_load_parameter_from_disk(
        machine_, params_dirname.c_str()));
  }
}

void InferenceHelper::Infer(std::vector<int>& dims, int repeat) {
  int batch_size = dims[0];
  int input_channel = dims[1];
  int input_height = dims[2];
  int input_width = dims[3];
  int input_size = input_channel * input_height * input_width;

  // Create input arguments and matrix.
  paddle_arguments in_args = paddle_arguments_create_none();
  CHECK(paddle_arguments_resize(in_args, 1));
  CHECK(
      paddle_arguments_set_frame_shape(in_args, 0, input_height, input_width));
  paddle_matrix in_mat = paddle_matrix_create(/* sample_num */ 1,
                                              /* size */ input_size,
                                              /* useGPU */ use_gpu_);

  // Create output arguments and matrix.
  paddle_arguments out_args = paddle_arguments_create_none();
  paddle_matrix out_mat = paddle_matrix_create_none();

  // Prepare input data
  paddle_real* cpu_input =
      (paddle_real*)malloc(input_size * sizeof(paddle_real));

  srand(time(0));
  for (int i = 0; i < input_size; ++i) {
    cpu_input[i] = rand() / ((float)RAND_MAX);
  }

  CHECK(paddle_arguments_resize(in_args, 1));
  CHECK(paddle_arguments_set_value(in_args, 0, in_mat));
  CHECK(paddle_matrix_set_value(in_mat, cpu_input));
  CHECK(paddle_gradient_machine_forward(machine_, in_args, out_args,
                                        /* isTrain */ false));
  CHECK(paddle_arguments_get_value(out_args, 0, out_mat));
  uint64_t out_height = 0;
  uint64_t out_width = 0;
  CHECK(paddle_matrix_get_shape(out_mat, &out_height, &out_width));
  std::cout << "Result: " << out_height << " x " << out_width << std::endl;
  paddle_real* cpu_output = nullptr;
  if (out_height * out_width > 0) {
    cpu_output =
        (paddle_real*)malloc(out_height * out_width * sizeof(paddle_real));
    CHECK(paddle_matrix_get_value(out_mat, cpu_output));
  }

  {
    Timer time("paddle_gradient_machine_forward:" + std::to_string(repeat));
    for (int i = 0; i < repeat; i++) {
      paddle_arguments_resize(in_args, 1);
      paddle_arguments_set_value(in_args, 0, in_mat);
      paddle_matrix_set_value(in_mat, cpu_input);

      paddle_gradient_machine_forward(machine_, in_args, out_args,
                                      /* isTrain */ false);

      paddle_arguments_get_value(out_args, 0, out_mat);
      paddle_matrix_get_value(out_mat, cpu_output);
    }
  }

  CHECK(paddle_arguments_destroy(in_args));
  CHECK(paddle_arguments_destroy(out_args));
  CHECK(paddle_matrix_destroy(in_mat));
  CHECK(paddle_matrix_destroy(out_mat));

  free(cpu_input);
  free(cpu_output);
}

void InferenceHelper::Release() {
  CHECK(paddle_gradient_machine_destroy(machine_));
}
